{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "Autoencoder para FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de paquetes\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Torchvision\n",
    "import torchvision.utils\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset\n",
    "from torchvision.datasets import FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros de entrenamiento\n",
    "latent_dims es el tamaño del bottleneck del autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 10\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "capacity = 64\n",
    "learning_rate = 1e-3\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Dataset\n",
    "\n",
    "60.000 imágenes en 10 categorias de ropa: Top/T-shirt, Trouser, Pullover, Dress, Coat, Sandar, Shirt, Sneaker, Bag, Anckle Boot. Normalizamos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # image = (image - mean) / std\n",
    "])\n",
    "\n",
    "train_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=True, transform=img_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=False, transform=img_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Definition\n",
    "\n",
    "Vamos a usar una arquitectura encoder/decoder convolucional.\n",
    "En los layers convolucionales, incrementamos la cantidad de canales a medida que nos acercamos al bottleneck. A pesar de eso, el número total de features se reduce ya que la cantidad de canales se incrementa en un factor de 2, pero las dimensiones espaciales se reducen por un factor de 4 (https://distill.pub/2016/deconv-checkerboard/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        c = capacity\n",
    "        # TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        c = capacity\n",
    "        # TODO\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon\n",
    "    \n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de entrenamiento\n",
    "Para evaluar la calidad de la reconstrucción a cada paso, utilizamos MSE loss https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# red en modo entrenamiento\n",
    "autoencoder.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch, _ in train_dataloader:\n",
    "        \n",
    "        image_batch = image_batch.to(device)\n",
    "        \n",
    "        # reconstrucción del autoencoder\n",
    "        image_batch_recon = autoencoder(image_batch)\n",
    "        \n",
    "        # error de reconstrucción\n",
    "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimizamos los pesos usando el gradiente propagado por backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_loss_avg)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Reconstruction error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red en modo evaluación\n",
    "autoencoder.eval()\n",
    "\n",
    "test_loss_avg, num_batches = 0, 0\n",
    "for image_batch, _ in test_dataloader:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        image_batch = image_batch.to(device)\n",
    "\n",
    "        # reconstruimos con el autoencoder \n",
    "        image_batch_recon = autoencoder(image_batch)\n",
    "\n",
    "        # medimos el error de reconstruction \n",
    "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
    "\n",
    "        test_loss_avg += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "test_loss_avg /= num_batches\n",
    "print('average reconstruction error: %f' % (test_loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos algunas reconstrucciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "\n",
    "# Funciones de graficación de las imagenes\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    return x\n",
    "\n",
    "def show_image(img):\n",
    "    img = to_img(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def visualise_output(images, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        images = images.to(device)\n",
    "        images = model(images)\n",
    "        images = images.cpu()\n",
    "        images = to_img(images)\n",
    "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
    "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
    "        plt.show()\n",
    "\n",
    "images, labels = iter(test_dataloader).next()\n",
    "\n",
    "# Primero, mostramos las imágenes originales (GT)\n",
    "print('Original images')\n",
    "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
    "plt.show()\n",
    "\n",
    "# Visualizamos las reconstrucciones del autoencoder\n",
    "print('Autoencoder reconstruction:')\n",
    "visualise_output(images, autoencoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
