{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "Autoencoder para FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de paquetes\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Torchvision\n",
    "import torchvision.utils\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset\n",
    "from torchvision.datasets import FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros de entrenamiento\n",
    "latent_dims es el tamaño del bottleneck del autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 2\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "capacity = 64\n",
    "learning_rate = 1e-3\n",
    "variational_beta = 1\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Dataset\n",
    "\n",
    "60.000 imágenes en 10 categorias de ropa: Top/T-shirt, Trouser, Pullover, Dress, Coat, Sandar, Shirt, Sneaker, Bag, Anckle Boot. Normalizamos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # image = (image - mean) / std\n",
    "])\n",
    "\n",
    "train_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=True, transform=img_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=False, transform=img_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Definition\n",
    "\n",
    "Vamos a usar una arquitectura encoder/decoder convolucional.\n",
    "En los layers convolucionales, incrementamos la cantidad de canales a medida que nos acercamos al bottleneck. A pesar de eso, el número total de features se reduce ya que la cantidad de canales se incrementa en un factor de 2, pero las dimensiones espaciales se reducen por un factor de 4 (https://distill.pub/2016/deconv-checkerboard/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 308357\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
    "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
    "        self.fc_mu = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
    "        self.fc_logvar = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1) # aplastamos el tensor\n",
    "        x_mu = self.fc_mu(x)\n",
    "        x_logvar = self.fc_logvar(x)\n",
    "        return x_mu, x_logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), capacity*2, 7, 7) # transformamos el tensor a 4D\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv1(x)) # Sigmoide (estamos usando una loss BCE)\n",
    "        return x\n",
    "    \n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encoder(x)\n",
    "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon, latent_mu, latent_logvar\n",
    "    \n",
    "    def latent_sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # reparameterización\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "# Definición de loss\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "\n",
    "    #Promediar o no la entropía cruzada binaria sobre todos los píxeles\n",
    "    #es un detalle sutil, pero con fuerte impacto en el entrenamiento, porque cambia\n",
    "    #la magnitud de los pesos que tenemos que elegir para los otros términos de la \n",
    "    #loss por varios órdenes de magnitud. No promediarlos es una implementación\n",
    "    #directa de -log likelihood, pero promediarlos hace que los pesos de los\n",
    "    #otros términos de la los sean independientes de la resolución de la imagen\n",
    "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "    \n",
    "    # KL-divergence sobre el espacio latente\n",
    "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return recon_loss + variational_beta * kldivergence\n",
    "    \n",
    "    \n",
    "vae = VariationalAutoencoder()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "vae = vae.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de entrenamiento\n",
    "Para evaluar la calidad de la reconstrucción a cada paso, utilizamos MSE loss https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# red en modo entrenamiento\n",
    "vae.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch, _ in train_dataloader:\n",
    "        \n",
    "        image_batch = image_batch.to(device)\n",
    "        \n",
    "        # reconstrucción del vae\n",
    "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
    "        \n",
    "        # error de reconstrucción\n",
    "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimizamos los pesos usando el gradiente propagado por backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_loss_avg)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red en modo evaluación\n",
    "vae.eval()\n",
    "\n",
    "test_loss_avg, num_batches = 0, 0\n",
    "for image_batch, _ in test_dataloader:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        image_batch = image_batch.to(device)\n",
    "\n",
    "        # vae \n",
    "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
    "\n",
    "        #  error\n",
    "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
    "\n",
    "        test_loss_avg += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "test_loss_avg /= num_batches\n",
    "print('average reconstruction error: %f' % (test_loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos algunas reconstrucciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "\n",
    "# Funciones de graficación de las imagenes\n",
    "def to_img(x):\n",
    "    x = x.clamp(0, 1)\n",
    "    return x\n",
    "\n",
    "def show_image(img):\n",
    "    img = to_img(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def visualise_output(images, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        images = images.to(device)\n",
    "        images, _, _ = model(images)\n",
    "        images = images.cpu()\n",
    "        images = to_img(images)\n",
    "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
    "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
    "        plt.show()\n",
    "\n",
    "images, labels = iter(test_dataloader).next()\n",
    "\n",
    "# Primero, mostramos las imágenes originales (GT)\n",
    "print('Original images')\n",
    "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
    "plt.show()\n",
    "\n",
    "# Visualizamos las reconstrucciones del autoencoder\n",
    "print('VAE reconstruction:')\n",
    "visualise_output(images, vae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
